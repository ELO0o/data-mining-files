{"nbformat":4,"nbformat_minor":0,"metadata":{"colab":{"provenance":[],"authorship_tag":"ABX9TyPUeXAaf4qdlG4AVaXlzFoi"},"kernelspec":{"name":"python3","display_name":"Python 3"},"language_info":{"name":"python"}},"cells":[{"cell_type":"code","source":["from google.colab import drive\n","drive.mount('/content/drive')"],"metadata":{"id":"nYZ2lPmmEfKU"},"execution_count":null,"outputs":[]},{"cell_type":"code","execution_count":null,"metadata":{"id":"vf9Lt_UtESQm"},"outputs":[],"source":["import numpy as np\n","import pandas as pd\n","import torch\n","import torch.nn as nn\n","import torch.nn.functional as F\n","import matplotlib.pyplot as plt\n","import torch.optim as optim\n","import random"]},{"cell_type":"markdown","source":["* unique\n","* is_in with sample\n","* concat"],"metadata":{"id":"XRuJ819sISiq"}},{"cell_type":"code","source":["\"\"\"\n","LABELS:\n","\n","ALL: Benign\n","\n","2-14:\n","]FTP-BruteForce\n","SSH-BruteForce\n","\n","2-15:\n","DoS attacks-Goldeneye\n","DoS attacks-Slowloris\n","\n","2-16:\n","DoS attacks-Hulk\n","DoS attacks-SlowHTTPTest\n","\n","2-21:\n","DDoS attack-LOIC-UDP\n","DDoS attack-HOIC\n","\n","2-22:\n","Brute Force -Web\n","Brute Force -XSS\n","SQL Injection\n","\n","2-23:\n","Brute Force -Web\n","Brute Force -XSS\n","SQL Injection\n","\n","2-28:\n","Infiltration\n","Label (???)\n","\n","3-01:\n","Infiltration\n","Label (???)\n","\n","3-02:\n","Bot\n","\n","\"\"\""],"metadata":{"id":"qUX7JIlFGBMV"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["#Concat"],"metadata":{"id":"tRdpHjlfFQBE"}},{"cell_type":"code","source":["# make 3 seperate datasets for 3 feature labels\n","data_1 = cleaned_data[cleaned_data['Label'] == 0]\n","data_2 = cleaned_data[cleaned_data['Label'] == 1]\n","data_3 = cleaned_data[cleaned_data['Label'] == 2]\n","\n","# make benign feature\n","y_1 = np.zeros(data_1.shape[0])\n","y_benign = pd.DataFrame(y_1)\n","\n","# make bruteforce feature\n","y_2 = np.ones(data_2.shape[0])\n","y_bf = pd.DataFrame(y_2)\n","\n","# make bruteforceSSH feature\n","y_3 = np.full(data_3.shape[0], 2)\n","y_ssh = pd.DataFrame(y_3)\n","\n","# merging the original dataframe\n","X = pd.concat([data_1, data_2, data_3], sort=True)\n","y = pd.concat([y_benign, y_bf, y_ssh], sort=True)"],"metadata":{"id":"Ew3IA2-PFj5j"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["#Value counts"],"metadata":{"id":"GiInpSkGQCUb"}},{"cell_type":"code","source":["# check the number of values for labels\n","df['Label'].value_counts()"],"metadata":{"id":"-RIvxb0DQFvF"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["#column amount and names, quantity, null, dtype,\n","df.info()"],"metadata":{"id":"9HBRxAVSJQIC"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["# reset index, replace, drop columns"],"metadata":{"id":"xvKothDEPPzM"}},{"cell_type":"code","source":["#drop label columns\n","x_train = x_train.drop(['is_Normal', 'is_Attack'], axis=1)"],"metadata":{"id":"YouQx_PzNJaE"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["#replace inf vals with nan vals\n","x_train.replace([np.inf, -np.inf], np.nan, inplace=True)\n","\n","#replace nan vals with 999\n","x_train.fillna(999, inplace=True)"],"metadata":{"id":"0rtfzA5QNSOh"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["df.sort_values(by=['item_price'], ascending=False).reset_index()"],"metadata":{"id":"4boOrrYuKJRV"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["#astype"],"metadata":{"id":"ftgRD-tkPe_2"}},{"cell_type":"code","source":["#creates a new column using item_price, reads all values, turns into float\n","df['new_price'] = df['item_price'].str[1:].astype(float)"],"metadata":{"id":"U7XONUywKIoa"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["# misc\n","* concatenation (merge)"],"metadata":{"id":"eoBp0pnwPn1K"}},{"cell_type":"code","source":["#applying standardscaler to train\n","from sklearn.preprocessing import StandardScaler\n","\n","scaler = StandardScaler()\n","x_train = scaler.fit_transform(x_train)\n","x_val = scaler.transform(x_val)\n","x_test = scaler.transform(x_test)"],"metadata":{"id":"L_tYajUcNuzg"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["#Find the number of survivors by passenger class and the survival rate by passenger class\n","df.groupby('Pclass').Survived.value_counts()\n","\n","df.groupby('Pclass').Survived.mean()"],"metadata":{"id":"fRhz77iIKquV"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["Question #15: Make two data frames first. The first data frame consists of PassengerId and Name columns. The second data frame consists of Name, SibSp, Parch, and Embarked. Then, merge these two data frames consisting of PassengerId, Name, SibSp, Parch, and Embarked. Find how many passengers were on board alone and with family members.\n"],"metadata":{"id":"dom8q3KgMEoF"}},{"cell_type":"code","source":["df1 = df[['Name', 'PassengerId']]\n","df2 = df[['Name', 'SibSp', 'Parch', 'Embarked']]\n","df3 = pd.merge(df1, df2)\n","df3"],"metadata":{"id":"oPCDfcLSKxLf"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["df3['FamilySize'] = len(df3.loc[(df.SibSp == 0) + (df3.Parch == 0)])\n","#family = len(df3.loc[(df.SibSp != 0) | (df3.Parch != 0)]) - 2   #for 2 null values\n","#print(\"alone: \", alone, \"\\nfamily: \", family)\n","df3"],"metadata":{"id":"wQO0z-CyKxNv"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["#Preprocessing (using CNN code)\n","* unique()\n","* value_counts()\n","* isna()\n","* concat()"],"metadata":{"id":"2ODF189PRZCx"}},{"cell_type":"code","source":["# check for some null or missing values in our dataset\n","df.isna().sum().to_numpy()"],"metadata":{"id":"z0qVU5kLKxki"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["# drop null or missing columns\n","cleaned_data = df.dropna()\n","cleaned_data.isna().sum().to_numpy()"],"metadata":{"id":"aDQIrwcNSVUX"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["# encode the column labels\n","label_encoder = LabelEncoder()\n","cleaned_data['Label']= label_encoder.fit_transform(cleaned_data['Label'])\n","cleaned_data['Label'].unique()"],"metadata":{"id":"IiEt9esaKxmy"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["# check for encoded labels\n","cleaned_data['Label'].value_counts()"],"metadata":{"id":"EbNdjsNASAxQ"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["# make 3 seperate datasets for 3 feature labels\n","data_1 = cleaned_data[cleaned_data['Label'] == 0]\n","data_2 = cleaned_data[cleaned_data['Label'] == 1]\n","data_3 = cleaned_data[cleaned_data['Label'] == 2]\n","\n","# make benign feature\n","y_1 = np.zeros(data_1.shape[0])\n","y_benign = pd.DataFrame(y_1)\n","\n","# make bruteforce feature\n","y_2 = np.ones(data_2.shape[0])\n","y_bf = pd.DataFrame(y_2)\n","\n","# make bruteforceSSH feature\n","y_3 = np.full(data_3.shape[0], 2)\n","y_ssh = pd.DataFrame(y_3)\n","\n","# merging the original dataframe\n","X = pd.concat([data_1, data_2, data_3], sort=True)\n","y = pd.concat([y_benign, y_bf, y_ssh], sort=True)"],"metadata":{"id":"b0irAiafSAua"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["# checking if there are some null values in data\n","X.isnull().sum().to_numpy()"],"metadata":{"id":"9HGinnd_SAq9"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["data augmentation"],"metadata":{"id":"_CtPkbQLTWBJ"}},{"cell_type":"code","source":["from sklearn.utils import resample\n","\n","data_1_resample = resample(data_1, n_samples=20000,\n","                           random_state=123, replace=True)\n","data_2_resample = resample(data_2, n_samples=20000,\n","                           random_state=123, replace=True)\n","data_3_resample = resample(data_3, n_samples=20000,\n","                           random_state=123, replace=True)"],"metadata":{"id":"MIwmMorjTZQd"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["train_dataset = pd.concat([data_1_resample, data_2_resample, data_3_resample])\n","train_dataset.head(2)"],"metadata":{"id":"bdiSzG3PTacI"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["# other preprrocessing"],"metadata":{"id":"tNGH6FSvWzhV"}},{"cell_type":"code","source":["df_main[\"Protocol\"].unique()"],"metadata":{"id":"Ryrvr0T-Tae4"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["df_main[\"Label\"].unique()"],"metadata":{"id":"Y-i7vb_uTahc"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["df_main.iloc[:,-1].value_counts()"],"metadata":{"id":"cIXKYygMTakD"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["# Remove row with Label[\"Label\"]\n","df_main.drop(df_main.loc[df_main[\"Label\"] == \"Label\"].index, inplace=True)"],"metadata":{"id":"gyNkgFQsYRaK"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["# Replace inf values to nan\n","df_main = df_main.replace([np.inf, -np.inf], np.nan)\n","# Count nan values\n","df_main.isna().sum().sum()"],"metadata":{"id":"FoUE3I4YYRcy"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["# Drop nan values\n","df_main.dropna(inplace=True)\n","df_main.isna().sum().sum()\n","df_main.shape"],"metadata":{"id":"v9j6u6UwYRfa"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["# Drop duplicate values\n","df_main.drop_duplicates(inplace=True)\n","df_main.shape"],"metadata":{"id":"_Uu90JZ6YRhu"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["df_main['Label'] = df_main['Label'].apply(lambda x: 0 if x.startswith(\"Benign\") else 1)\n","df_main['Label'].value_counts()"],"metadata":{"id":"2jTPwg7NYgYr"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["# Remove timestamp columns\n","df_main.drop(['Timestamp'], axis=1,inplace=True)"],"metadata":{"id":"pMWo_BTBYgbH"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["#can try w astype instead\n","\n","def convert_to_numeric(df):\n","  \"\"\"Converts all features (except label) from object to float64 or int64.\n","  Args:\n","      df: A pandas DataFrame.\n","\n","  Returns:\n","      A DataFrame with features converted to float64 (if possible).\n","  \"\"\"\n","  # Select all columns except the label column (assuming 'Label' is the name)\n","  numeric_cols = df.columns.difference(['Dst Port', 'Protocol', 'Label'])\n","  # Try converting each column to float, ignoring errors for non-numeric values\n","  for col in numeric_cols:\n","    try:\n","      df[col] = pd.to_numeric(df[col], errors='coerce')\n","    except:\n","      pass\n","  return df\n","\n","df_numeric = convert_to_numeric(df_main.copy())\n","del df_main\n","gc.collect()"],"metadata":{"id":"SwWKHM31Ygd_"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["# Replace inf values to nan\n","df_numeric = df_numeric.replace([np.inf, -np.inf], np.nan)\n","# Count nan values\n","print(df_numeric.isna().sum().sum())\n","df_numeric.dropna(inplace=True)"],"metadata":{"id":"0UxvzQO5ZJyG"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["# Drop constant columns\n","variances = df_numeric.var(numeric_only=True)\n","constant_columns = variances[variances == 0].index\n","df_numeric.drop(constant_columns, axis=1, inplace=True)\n","\n","print(constant_columns)\n","print(df_numeric.shape)"],"metadata":{"id":"hp06TI8O8TmU"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["print(df_numeric.shape)\n","print(df_numeric['Label'].value_counts())"],"metadata":{"id":"CSvXRf398V_z"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["# Process Protocol columns\n","df_numeric = df_numeric.astype({\"Protocol\": str})\n","df_numeric[\"Protocol\"].unique()"],"metadata":{"id":"a_bobbMX8WB5"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["# Categorical data to onehot\n","df_numeric = pd.get_dummies(df_numeric, columns=['Protocol'])\n","df_numeric.head()"],"metadata":{"id":"Uhbg6NH18WEG"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["gc.collect()"],"metadata":{"id":"LvbQPQLxKe3j"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["# making Label column the last column again\n","df_numeric.insert(len(df_numeric.columns)-1, 'Label', df_numeric.pop('Label'))"],"metadata":{"id":"_3q26RkfKe55"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["df_numeric = df_numeric.astype({\"Protocol_0\": 'int64', \"Protocol_17\": 'int64', \"Protocol_6\": 'int64'})\n","df_numeric.head()"],"metadata":{"id":"Mak656FlKe8C"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["df_numeric = df_numeric.astype({\"Dst Port\": str})\n","\n","df_numeric = df_numeric.astype({\"Dst Port\": 'int64'})\n","df_numeric.info()"],"metadata":{"id":"TvQiQqecKe-d"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["#Optuna"],"metadata":{"id":"_oipcNL96bLv"}},{"cell_type":"code","source":["import optuna\n","from sklearn.model_selection import cross_val_score"],"metadata":{"id":"0ubtIN1n6c-L"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["#objective\n","def objective(trial):\n","  n_estimators = trial.suggest_int('n_estimators', 100,1000) #range of numbers to test\n","  max_depth = trial.suggest_int('max_depth', 100, 50)\n","  min_samples_split = trial.suggest_int('min_samples_split', 2,32)\n","  min_samples_leaf =trial.suggest_int('min_samples_leaf', 1, 32)\n","\n","  #or name of other model, inside are parameter names\n","  model = RandomForestRegressor(n_estimators = n_estimators,\n","                                max_depth = max_depth,\n","                                min_samples_split = min_samples_split,\n","                                min_samples_leaf = min_samples_leaf)\n","\n","  score = cross_val_score(model, x_train, y_train, cv=5, scoring='neg_mean_squared', n_jobs = -1, )\n","\n","  return score"],"metadata":{"colab":{"base_uri":"https://localhost:8080/","height":105},"id":"zxfyXcZQ6dSa","executionInfo":{"status":"error","timestamp":1714495898929,"user_tz":300,"elapsed":95,"user":{"displayName":"Ethan Lopez","userId":"06782970102147101246"}},"outputId":"83606d21-6ad1-4dee-ab81-2ab9f8634f99"},"execution_count":null,"outputs":[{"output_type":"error","ename":"SyntaxError","evalue":"incomplete input (<ipython-input-1-efaf128e5c80>, line 2)","traceback":["\u001b[0;36m  File \u001b[0;32m\"<ipython-input-1-efaf128e5c80>\"\u001b[0;36m, line \u001b[0;32m2\u001b[0m\n\u001b[0;31m    def objective(trial):\u001b[0m\n\u001b[0m                         ^\u001b[0m\n\u001b[0;31mSyntaxError\u001b[0m\u001b[0;31m:\u001b[0m incomplete input\n"]}]},{"cell_type":"code","source":["study = optuna.create_study(direction='maximize', sampler=optuna.samplers.RandomSampler, seed=777)"],"metadata":{"id":"dYeU8_SO6dU1"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["study.optimize(objective, n_trials=200)"],"metadata":{"id":"mOhVVma16dXQ"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["study.best_params"],"metadata":{"id":"D7U8Fxhc6dbT"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["best_params = study.best_params"],"metadata":{"id":"oO5OpC6j6dc4"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["import matplotlib.pyplot as plt"],"metadata":{"id":"am3XN0uG-o87"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["optuna.visualization.plot_optimization_history(study)"],"metadata":{"id":"38NxlszL-o_W"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["optuna.visualization.plot_parallel_coordinate(study)"],"metadata":{"id":"DoldKQ4t-pBv"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["optuna.visualization.plot_slice(study, params=['n_estimators', 'max_depth', 'min_sample_split', 'min_sample_leaf'])"],"metadata":{"id":"_hTq4WmI-pED"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["optuna.visualization.plot_param_importances(study)"],"metadata":{"id":"cvtKftwg-pGq"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["#creating new model\n","\n","best_n_estimators = best_params['n_estimators']\n","best_max_depth = best_params['max_depth']\n","best_min_sample_split = best_params['min_sample_split']\n","best_n_min_sample_leaf = best_params['min_sample_leaf']"],"metadata":{"id":"3H8pJ8bd_6-q"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["best_model = RandomForestRegressor(n_estimators = best_n_estimators\n","                                  max_depth = best_max_depth\n","                                  min_sample_split = best_min_sample_split\n","                                  best_min_sample_leaf = best_min_sample_leaf)"],"metadata":{"id":"LS5zNhn6AcG9"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["#Tensor Calculator"],"metadata":{"id":"DbhAaWNjFnQj"}},{"cell_type":"code","source":["import torch\n","import torch.nn as nn"],"metadata":{"id":"H142bpcBFpcK"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["#layer 1\n","\n","input = torch.Tensor(100, 1, 1000, 1000) #batch_size: 100, input_channel: 1, input_size: 28x28\n","\n","conv = torch.nn.Conv1d(in_channels= 1, out_channels= 1, kernel_size= 2, stride= 2, padding= 0)\n","\n","pool = torch.nn.MaxPool1d(kernel_size=1, stride=2)\n","\n","\n","out = conv(input)\n","out = pool(out)\n","out.size()\n","\n","#input -> conv -> pool -> out"],"metadata":{"id":"Q2mLwVUhFqOs"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["#1D CNN Ex"],"metadata":{"id":"-uES49r0YX5M"}},{"cell_type":"code","source":["df_train = pd.read_csv(\"../input/liverpool-ion-switching/train.csv\")\n","df_test = pd.read_csv(\"../input/liverpool-ion-switching/test.csv\")\n","\n","# I don't use \"time\" feature\n","train_input = df_train[\"signal\"].values.reshape(-1,4000,1)#number_of_data:1250 x time_step:4000\n","train_input_mean = train_input.mean()\n","train_input_sigma = train_input.std()\n","train_input = (train_input-train_input_mean)/train_input_sigma\n","test_input = df_test[\"signal\"].values.reshape(-1,10000,1)\n","test_input = (test_input-train_input_mean)/train_input_sigma\n","\n","train_target = pd.get_dummies(df_train[\"open_channels\"]).values.reshape(-1,4000,11)#classification\n","\n","idx = np.arange(train_input.shape[0])\n","train_idx, val_idx = train_test_split(idx, random_state = 111,test_size = 0.2)\n","\n","val_input = train_input[val_idx]\n","train_input = train_input[train_idx]\n","val_target = train_target[val_idx]\n","train_target = train_target[train_idx]\n","\n","print(\"train_input:{}, val_input:{}, train_target:{}, val_target:{}\".format(train_input.shape, val_input.shape, train_target.shape, val_target.shape))"],"metadata":{"id":"IMu4ni4ZYavd"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["import numpy\n","import torch\n","\n","X = numpy.random.uniform(-10, 10, 70).reshape(1, 7, -1)\n","# Y = np.random.randint(0, 9, 10).reshape(1, 1, -1)\n","\n","class Simple1DCNN(torch.nn.Module):\n","    def __init__(self):\n","        super(Simple1DCNN, self).__init__()\n","        self.layer1 = torch.nn.Conv1d(in_channels=7, out_channels=20, kernel_size=5, stride=2)\n","        self.act1 = torch.nn.ReLU()\n","        self.layer2 = torch.nn.Conv1d(in_channels=20, out_channels=10, kernel_size=1)\n","    def forward(self, x):\n","        x = self.layer1(x)\n","        x = self.act1(x)\n","        x = self.layer2(x)\n","\n","        log_probs = torch.nn.functional.log_softmax(x, dim=1)\n","\n","        return log_probs\n","\n","model = Simple1DCNN().double()\n","print(model(torch.tensor(X)).shape)"],"metadata":{"id":"rog6ssilY1xQ"},"execution_count":null,"outputs":[]}]}